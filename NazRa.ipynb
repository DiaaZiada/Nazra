{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NazRa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiaaZiada/Nazra/blob/master/NazRa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad_rpMFphll3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b3302fc1-1130-4421-a09b-37207459ce9f"
      },
      "source": [
        "!git clone https://github.com/DiaaZiada/Nazra.git\n",
        "!mv Nazra/data data/\n",
        "!mv Nazra/Model Model\n",
        "!rm -r Nazra"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Nazra'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/99)   \u001b[K\rremote: Counting objects:   2% (2/99)   \u001b[K\rremote: Counting objects:   3% (3/99)   \u001b[K\rremote: Counting objects:   4% (4/99)   \u001b[K\rremote: Counting objects:   5% (5/99)   \u001b[K\rremote: Counting objects:   6% (6/99)   \u001b[K\rremote: Counting objects:   7% (7/99)   \u001b[K\rremote: Counting objects:   8% (8/99)   \u001b[K\rremote: Counting objects:   9% (9/99)   \u001b[K\rremote: Counting objects:  10% (10/99)   \u001b[K\rremote: Counting objects:  11% (11/99)   \u001b[K\rremote: Counting objects:  12% (12/99)   \u001b[K\rremote: Counting objects:  13% (13/99)   \u001b[K\rremote: Counting objects:  14% (14/99)   \u001b[K\rremote: Counting objects:  15% (15/99)   \u001b[K\rremote: Counting objects:  16% (16/99)   \u001b[K\rremote: Counting objects:  17% (17/99)   \u001b[K\rremote: Counting objects:  18% (18/99)   \u001b[K\rremote: Counting objects:  19% (19/99)   \u001b[K\rremote: Counting objects:  20% (20/99)   \u001b[K\rremote: Counting objects:  21% (21/99)   \u001b[K\rremote: Counting objects:  22% (22/99)   \u001b[K\rremote: Counting objects:  23% (23/99)   \u001b[K\rremote: Counting objects:  24% (24/99)   \u001b[K\rremote: Counting objects:  25% (25/99)   \u001b[K\rremote: Counting objects:  26% (26/99)   \u001b[K\rremote: Counting objects:  27% (27/99)   \u001b[K\rremote: Counting objects:  28% (28/99)   \u001b[K\rremote: Counting objects:  29% (29/99)   \u001b[K\rremote: Counting objects:  30% (30/99)   \u001b[K\rremote: Counting objects:  31% (31/99)   \u001b[K\rremote: Counting objects:  32% (32/99)   \u001b[K\rremote: Counting objects:  33% (33/99)   \u001b[K\rremote: Counting objects:  34% (34/99)   \u001b[K\rremote: Counting objects:  35% (35/99)   \u001b[K\rremote: Counting objects:  36% (36/99)   \u001b[K\rremote: Counting objects:  37% (37/99)   \u001b[K\rremote: Counting objects:  38% (38/99)   \u001b[K\rremote: Counting objects:  39% (39/99)   \u001b[K\rremote: Counting objects:  40% (40/99)   \u001b[K\rremote: Counting objects:  41% (41/99)   \u001b[K\rremote: Counting objects:  42% (42/99)   \u001b[K\rremote: Counting objects:  43% (43/99)   \u001b[K\rremote: Counting objects:  44% (44/99)   \u001b[K\rremote: Counting objects:  45% (45/99)   \u001b[K\rremote: Counting objects:  46% (46/99)   \u001b[K\rremote: Counting objects:  47% (47/99)   \u001b[K\rremote: Counting objects:  48% (48/99)   \u001b[K\rremote: Counting objects:  49% (49/99)   \u001b[K\rremote: Counting objects:  50% (50/99)   \u001b[K\rremote: Counting objects:  51% (51/99)   \u001b[K\rremote: Counting objects:  52% (52/99)   \u001b[K\rremote: Counting objects:  53% (53/99)   \u001b[K\rremote: Counting objects:  54% (54/99)   \u001b[K\rremote: Counting objects:  55% (55/99)   \u001b[K\rremote: Counting objects:  56% (56/99)   \u001b[K\rremote: Counting objects:  57% (57/99)   \u001b[K\rremote: Counting objects:  58% (58/99)   \u001b[K\rremote: Counting objects:  59% (59/99)   \u001b[K\rremote: Counting objects:  60% (60/99)   \u001b[K\rremote: Counting objects:  61% (61/99)   \u001b[K\rremote: Counting objects:  62% (62/99)   \u001b[K\rremote: Counting objects:  63% (63/99)   \u001b[K\rremote: Counting objects:  64% (64/99)   \u001b[K\rremote: Counting objects:  65% (65/99)   \u001b[K\rremote: Counting objects:  66% (66/99)   \u001b[K\rremote: Counting objects:  67% (67/99)   \u001b[K\rremote: Counting objects:  68% (68/99)   \u001b[K\rremote: Counting objects:  69% (69/99)   \u001b[K\rremote: Counting objects:  70% (70/99)   \u001b[K\rremote: Counting objects:  71% (71/99)   \u001b[K\rremote: Counting objects:  72% (72/99)   \u001b[K\rremote: Counting objects:  73% (73/99)   \u001b[K\rremote: Counting objects:  74% (74/99)   \u001b[K\rremote: Counting objects:  75% (75/99)   \u001b[K\rremote: Counting objects:  76% (76/99)   \u001b[K\rremote: Counting objects:  77% (77/99)   \u001b[K\rremote: Counting objects:  78% (78/99)   \u001b[K\rremote: Counting objects:  79% (79/99)   \u001b[K\rremote: Counting objects:  80% (80/99)   \u001b[K\rremote: Counting objects:  81% (81/99)   \u001b[K\rremote: Counting objects:  82% (82/99)   \u001b[K\rremote: Counting objects:  83% (83/99)   \u001b[K\rremote: Counting objects:  84% (84/99)   \u001b[K\rremote: Counting objects:  85% (85/99)   \u001b[K\rremote: Counting objects:  86% (86/99)   \u001b[K\rremote: Counting objects:  87% (87/99)   \u001b[K\rremote: Counting objects:  88% (88/99)   \u001b[K\rremote: Counting objects:  89% (89/99)   \u001b[K\rremote: Counting objects:  90% (90/99)   \u001b[K\rremote: Counting objects:  91% (91/99)   \u001b[K\rremote: Counting objects:  92% (92/99)   \u001b[K\rremote: Counting objects:  93% (93/99)   \u001b[K\rremote: Counting objects:  94% (94/99)   \u001b[K\rremote: Counting objects:  95% (95/99)   \u001b[K\rremote: Counting objects:  96% (96/99)   \u001b[K\rremote: Counting objects:  97% (97/99)   \u001b[K\rremote: Counting objects:  98% (98/99)   \u001b[K\rremote: Counting objects: 100% (99/99)   \u001b[K\rremote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 248 (delta 47), reused 84 (delta 34), pack-reused 149\u001b[K\n",
            "Receiving objects: 100% (248/248), 2.67 MiB | 9.12 MiB/s, done.\n",
            "Resolving deltas: 100% (102/102), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBFsNxD7rrcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xhA8Z90X945",
        "colab_type": "code",
        "outputId": "0c03fb73-10bf-4606-9611-cce93d34b87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue May  7 20:19:52 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P8    18W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3tccHowZJCX",
        "colab_type": "code",
        "outputId": "434ce22a-8ad5-4325-ab7a-7ea3f90c3edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<torch.cuda.device object at 0x7f3749582358>\n",
            "1\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKK4Gw4qlDnc",
        "colab_type": "code",
        "outputId": "ba4b929b-bfff-43b1-a2af-2b4ab5990d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74wXStLm5SqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import time\n",
        "\n",
        "# https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/2\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std, img=False):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.img = img\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "            # The normalize code -> t.sub_(m).div_(s)\n",
        "            if self.img:\n",
        "                t.mul_(255)\n",
        "        return tensor\n",
        "\n",
        "def cudafy(object, use_cuda):\n",
        "    if use_cuda:\n",
        "        return object.cuda()\n",
        "    else:\n",
        "        return object\n",
        "\n",
        "def display_est_time_loop(tot_toc, curr_ix, tot_iter, prefix=''):\n",
        "    if curr_ix == tot_iter:\n",
        "        neat_time = time.strftime('%H:%M:%S', time.gmtime(tot_toc))\n",
        "        print(\"\\r\" + prefix + 'Total elapsed time (HH:MM:SS): ' + neat_time, end='')\n",
        "        print('')\n",
        "    else:\n",
        "        avg_toc = tot_toc / curr_ix\n",
        "        estimated_time_hours = (avg_toc * (tot_iter - curr_ix))\n",
        "        neat_time = time.strftime('%H:%M:%S', time.gmtime(estimated_time_hours))\n",
        "        perc = str(round(curr_ix*100/tot_iter))\n",
        "        print('\\r' + prefix + 'Estimated time (HH:MM:SS): ' + neat_time + ' ' + perc + '%', end='')\n",
        "    return tot_toc\n",
        "\n",
        "def _print_layer_output_shape(layer_name, output_shape):\n",
        "    print(\"Layer \" + layer_name + \" output shape: \" + str(output_shape))\n",
        "\n",
        "def swap_cols(arr, frm, to):\n",
        "    arr[:,[frm, to]] = arr[:,[to, frm]]\n",
        "\n",
        "def myprint(msg, filepath=None):\n",
        "    print(msg)\n",
        "    if not filepath is None:\n",
        "        with open(filepath, 'a') as f:\n",
        "            f.write(msg + '\\n')\n",
        "\n",
        "def list_files_in_dir(dir):\n",
        "    return [f for f in listdir(dir) if isfile(join(dir, f))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIDHe06q_RDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def NetBlockConvBatchRelu(in_channels, filters, kernel_size, stride,\n",
        "                          padding=0):\n",
        "    return nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=filters,\n",
        "                      kernel_size=kernel_size, stride=stride,\n",
        "                      padding=padding),\n",
        "            nn.BatchNorm2d(num_features=filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "def NetBlocksSequenceConvBatchRelu(num_blocks, in_channels, out_channels,\n",
        "                                   kernel_sizes, strides, paddings=0):\n",
        "    if paddings == 0:\n",
        "        paddings = [0] * num_blocks\n",
        "    conv_sequence = []\n",
        "    conv_sequence.append(NetBlockConvBatchRelu(in_channels, out_channels[0],\n",
        "                                               kernel_sizes[0], strides[0],\n",
        "                                               padding=paddings[0]))\n",
        "    for i in range(num_blocks - 1):\n",
        "        conv_sequence.append(NetBlockConvBatchRelu(out_channels[i], out_channels[i+1],\n",
        "                                                   kernel_sizes[i+1], strides[i+1],\n",
        "                                                   paddings[i+1]))\n",
        "    conv_sequence = nn.Sequential(*conv_sequence)\n",
        "    return conv_sequence\n",
        "\n",
        "def NetBlocksDeconvBatchRelu(in_channels, out_channels, kernel_size, stride,\n",
        "                          padding=0):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                           stride=stride, padding=padding),\n",
        "        nn.BatchNorm2d(num_features=out_channels),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "def NetBlocksSequenceDeconvBatchRelu(num_blocks, in_channels, out_channels,\n",
        "                                   kernel_sizes, strides, paddings=0):\n",
        "    if paddings == 0:\n",
        "        paddings = [0] * num_blocks\n",
        "    deconv_sequence = []\n",
        "    deconv_sequence.append(NetBlocksDeconvBatchRelu(in_channels, out_channels[0],\n",
        "                                               kernel_sizes[0], strides[0],\n",
        "                                               padding=paddings[0]))\n",
        "    for i in range(num_blocks-1):\n",
        "        deconv_sequence.append(NetBlockConvBatchRelu(out_channels[i], out_channels[i+1],\n",
        "                                                   kernel_sizes[i+1], strides[i+1],\n",
        "                                                   paddings[i+1]))\n",
        "    deconv_sequence = nn.Sequential(*deconv_sequence)\n",
        "    return deconv_sequence\n",
        "\n",
        "class NetBlocksFlatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class NetBlockSoftmaxLogProbability2D(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetBlockSoftmaxLogProbability2D, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        orig_shape = x.data.shape\n",
        "        seq_x = []\n",
        "        for channel_ix in range(orig_shape[1]):\n",
        "            softmax_ = F.softmax(x[:, channel_ix, :, :].contiguous()\n",
        "                                 .view((orig_shape[0], orig_shape[2] * orig_shape[3])), dim=1)\\\n",
        "                .view((orig_shape[0], orig_shape[2], orig_shape[3]))\n",
        "            seq_x.append(softmax_.log())\n",
        "        x = torch.stack(seq_x, dim=1)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTLv6z1h_T7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def _print_layer_output_shape(layer_name, output_shape):\n",
        "    print(\"Layer \" + layer_name + \" output shape: \" + str(output_shape))\n",
        "\n",
        "def HALNetResConvSequence(stride, filters1, filters2,\n",
        "                          padding1=1, padding2=0, padding3=0,\n",
        "                          first_in_channels=0):\n",
        "    if first_in_channels == 0:\n",
        "        first_in_channels = filters1\n",
        "    return nn.Sequential(\n",
        "        # added padding = 1 to make shapes fit when joining\n",
        "        # with left module\n",
        "        NetBlockConvBatchRelu(kernel_size=1, stride=stride, filters=filters1,\n",
        "                              in_channels=first_in_channels, padding=padding1),\n",
        "        NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=filters1,\n",
        "                              in_channels=filters1, padding=padding2),\n",
        "        NetBlockConvBatchRelu(kernel_size=1, stride=1, filters=filters2,\n",
        "                              in_channels=filters1, padding=padding3)\n",
        "    )\n",
        "\n",
        "class HALNetResBlockIDSkip(nn.Module):\n",
        "    def __init__(self, filters1, filters2,\n",
        "                 padding_right1=1, padding_right2=0, padding_right3=0):\n",
        "        super(HALNetResBlockIDSkip, self).__init__()\n",
        "        self.right_res = HALNetResConvSequence(stride=1,\n",
        "                                               filters1=filters1,\n",
        "                                               filters2=filters2,\n",
        "                                               padding1=padding_right1,\n",
        "                                               padding2=padding_right2,\n",
        "                                               padding3=padding_right3,\n",
        "                                               first_in_channels=\n",
        "                                               filters2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        left_res = input\n",
        "        right_res = self.right_res(input)\n",
        "        # element-wise sum\n",
        "        out = left_res + right_res\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class HALNetResBlockConv(nn.Module):\n",
        "    def __init__(self, stride, filters1, filters2, first_in_channels=0,\n",
        "                 padding_left=0, padding_right1=0, padding_right2=0,\n",
        "                 padding_right3=0):\n",
        "        super(HALNetResBlockConv, self).__init__()\n",
        "        if first_in_channels == 0:\n",
        "            first_in_channels = filters1\n",
        "        self.left_res = NetBlockConvBatchRelu(kernel_size=1, stride=stride,\n",
        "                                              filters=filters2,\n",
        "                                              padding=padding_left,\n",
        "                                              in_channels=first_in_channels)\n",
        "        self.right_res = HALNetResConvSequence(stride=stride,\n",
        "                                               filters1=filters1,\n",
        "                                               filters2=filters2,\n",
        "                                               padding1=padding_right1,\n",
        "                                               padding2=padding_right2,\n",
        "\n",
        "                                               padding3=padding_right3,\n",
        "                                               first_in_channels=\n",
        "                                               first_in_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        left_res = self.left_res(input)\n",
        "        right_res = self.right_res(input)\n",
        "        # element-wise sum\n",
        "        out = left_res + right_res\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "def make_bilinear_weights(size, num_channels):\n",
        "    ''' Make a 2D bilinear kernel suitable for upsampling\n",
        "    Stack the bilinear kernel for application to tensor '''\n",
        "    factor = (size + 1) // 2\n",
        "    if size % 2 == 1:\n",
        "        center = factor - 1\n",
        "    else:\n",
        "        center = factor - 0.5\n",
        "    og = np.ogrid[:size, :size]\n",
        "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
        "           (1 - abs(og[1] - center) / factor)\n",
        "    filt = torch.from_numpy(filt)\n",
        "    w = torch.zeros(num_channels, 1, size, size)\n",
        "    for i in range(num_channels):\n",
        "        w[i, 0] = filt\n",
        "    return w\n",
        "\n",
        "def parse_model_param(params_dict, key, default_value):\n",
        "    try:\n",
        "        ret = params_dict[key]\n",
        "    except:\n",
        "        if default_value == \"Mandatory\":\n",
        "            # raise error again by trying to access value\n",
        "            ret = params_dict[key]\n",
        "        ret = default_value\n",
        "    return ret\n",
        "\n",
        "class HALNet(nn.Module):\n",
        "\n",
        "    cross_entropy = False\n",
        "    joint_ixs = None\n",
        "    use_cuda = None\n",
        "    WEIGHT_LOSS_INTERMED1 = 0.5\n",
        "    WEIGHT_LOSS_INTERMED2 = 0.5\n",
        "    WEIGHT_LOSS_INTERMED3 = 0.5\n",
        "    WEIGHT_LOSS_MAIN = 1\n",
        "\n",
        "    def __init__(self, params_dict):\n",
        "        super(HALNet, self).__init__()\n",
        "        # initialize variables\n",
        "        self.joint_ixs = parse_model_param(params_dict, 'joint_ixs', default_value=\"Mandatory\")\n",
        "        self.use_cuda = parse_model_param(params_dict, 'use_cuda', default_value=False)\n",
        "        self.num_joints = len(self.joint_ixs)\n",
        "        self.cross_entropy = parse_model_param(params_dict, 'cross_entropy', default_value=False)\n",
        "        # build network\n",
        "        self.conv1 = cudafy(NetBlockConvBatchRelu(kernel_size=7, stride=1, filters=64,\n",
        "                                                  in_channels=4, padding=3), self.use_cuda)\n",
        "        self.mp1 = cudafy(nn.MaxPool2d(kernel_size=3, stride=2, padding=1), self.use_cuda)\n",
        "        self.res2a = cudafy(HALNetResBlockConv(stride=1, filters1=64, filters2=256,\n",
        "                                        padding_right1=1), self.use_cuda)\n",
        "        self.res2b = cudafy(HALNetResBlockIDSkip(filters1=64, filters2=256), self.use_cuda)\n",
        "        self.res2c = cudafy(HALNetResBlockIDSkip(filters1=64, filters2=256), self.use_cuda)\n",
        "        self.res3a = cudafy(HALNetResBlockConv(stride=2, filters1=128, filters2=512,\n",
        "                                               padding_right3=1, first_in_channels=256), self.use_cuda)\n",
        "        self.interm_loss1 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=self.num_joints,\n",
        "                                                         in_channels=512, padding=1), self.use_cuda)\n",
        "        self.interm_loss1_deconv = cudafy(nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss1_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.res3b = cudafy(HALNetResBlockIDSkip(filters1=128, filters2=512), self.use_cuda)\n",
        "        self.res3c = cudafy(HALNetResBlockIDSkip(filters1=128, filters2=512), self.use_cuda)\n",
        "        self.res4a = cudafy(HALNetResBlockConv(stride=2, filters1=256, filters2=1024,\n",
        "                                        padding_right3=1,\n",
        "                                        first_in_channels=512), self.use_cuda)\n",
        "        self.interm_loss2 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                         filters=self.num_joints, in_channels=1024,\n",
        "                                                         padding=1), self.use_cuda)\n",
        "        self.interm_loss2_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss2_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.res4b = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.res4c = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.res4d = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.conv4e = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=512,\n",
        "                                                   in_channels=1024, padding=1), self.use_cuda)\n",
        "        self.interm_loss3 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                         filters=self.num_joints, in_channels=512,\n",
        "                                                         padding=1), self.use_cuda)\n",
        "        self.interm_loss3_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss3_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.conv4f = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=256,\n",
        "                                                   in_channels=512, padding=1), self.use_cuda)\n",
        "        self.main_loss_conv = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                           filters=self.num_joints, in_channels=256,\n",
        "                                                           padding=1), self.use_cuda)\n",
        "        self.main_loss_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        if self.cross_entropy:\n",
        "            self.softmax_final = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "\n",
        "    def forward_common_net(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.mp1(out)\n",
        "        out = self.res2a(out)\n",
        "        out = self.res2b(out)\n",
        "        out = self.res2c(out)\n",
        "        res3aout = self.res3a(out)\n",
        "        out = self.res3b(res3aout)\n",
        "        out = self.res3c(out)\n",
        "        res4aout = self.res4a(out)\n",
        "        out = self.res4b(res4aout)\n",
        "        out = self.res4c(out)\n",
        "        out = self.res4d(out)\n",
        "        conv4eout = self.conv4e(out)\n",
        "        conv4fout = self.conv4f(conv4eout)\n",
        "        return res3aout, res4aout, conv4eout, conv4fout\n",
        "\n",
        "    def forward_subnet(self, x):\n",
        "        res3aout, res4aout, conv4eout, conv4fout = self.forward_common_net(x)\n",
        "        # intermediate losses\n",
        "        # intermed 1\n",
        "        out_intermed1 = self.interm_loss1(res3aout)\n",
        "        out_intermed1 = self.interm_loss1_deconv(out_intermed1)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed1 = self.interm_loss1_softmax(out_intermed1)\n",
        "        # intermed 2\n",
        "        out_intermed2 = self.interm_loss2(res4aout)\n",
        "        out_intermed2 = self.interm_loss2_deconv(out_intermed2)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed2 = self.interm_loss2_softmax(out_intermed2)\n",
        "        # intermed 3\n",
        "        out_intermed3 = self.interm_loss3(conv4eout)\n",
        "        out_intermed3 = self.interm_loss3_deconv(out_intermed3)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed3 = self.interm_loss3_softmax(out_intermed3)\n",
        "        return out_intermed1, out_intermed2, out_intermed3, conv4fout,\\\n",
        "               res3aout, res4aout, conv4eout\n",
        "\n",
        "    def forward_main_loss(self, conv4fout):\n",
        "        out = self.main_loss_conv(conv4fout)\n",
        "        out = self.main_loss_deconv(out)\n",
        "        out_main = out\n",
        "        # main loss\n",
        "        if self.cross_entropy:\n",
        "            out_main = self.softmax_final(out)\n",
        "        return out_main\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get subhalnet outputs (common to JORNet)\n",
        "        out_intermed1, out_intermed2, out_intermed3, conv4fout, _, _, _ = self.forward_subnet(x)\n",
        "        # out to main loss of halnet\n",
        "        out_main = self.forward_main_loss(conv4fout)\n",
        "        return out_intermed1, out_intermed2, out_intermed3, out_main\n",
        "\n",
        "\n",
        "def _print_layer_output_shape(layer_name, output_shape):\n",
        "    print(\"Layer \" + layer_name + \" output shape: \" + str(output_shape))\n",
        "\n",
        "def HALNetResConvSequence(stride, filters1, filters2,\n",
        "                          padding1=1, padding2=0, padding3=0,\n",
        "                          first_in_channels=0):\n",
        "    if first_in_channels == 0:\n",
        "        first_in_channels = filters1\n",
        "    return nn.Sequential(\n",
        "        # added padding = 1 to make shapes fit when joining\n",
        "        # with left module\n",
        "        NetBlockConvBatchRelu(kernel_size=1, stride=stride, filters=filters1,\n",
        "                              in_channels=first_in_channels, padding=padding1),\n",
        "        NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=filters1,\n",
        "                              in_channels=filters1, padding=padding2),\n",
        "        NetBlockConvBatchRelu(kernel_size=1, stride=1, filters=filters2,\n",
        "                              in_channels=filters1, padding=padding3)\n",
        "    )\n",
        "\n",
        "class HALNetResBlockIDSkip(nn.Module):\n",
        "    def __init__(self, filters1, filters2,\n",
        "                 padding_right1=1, padding_right2=0, padding_right3=0):\n",
        "        super(HALNetResBlockIDSkip, self).__init__()\n",
        "        self.right_res = HALNetResConvSequence(stride=1,\n",
        "                                               filters1=filters1,\n",
        "                                               filters2=filters2,\n",
        "                                               padding1=padding_right1,\n",
        "                                               padding2=padding_right2,\n",
        "                                               padding3=padding_right3,\n",
        "                                               first_in_channels=\n",
        "                                               filters2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        left_res = input\n",
        "        right_res = self.right_res(input)\n",
        "        # element-wise sum\n",
        "        out = left_res + right_res\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class HALNetResBlockConv(nn.Module):\n",
        "    def __init__(self, stride, filters1, filters2, first_in_channels=0,\n",
        "                 padding_left=0, padding_right1=0, padding_right2=0,\n",
        "                 padding_right3=0):\n",
        "        super(HALNetResBlockConv, self).__init__()\n",
        "        if first_in_channels == 0:\n",
        "            first_in_channels = filters1\n",
        "        self.left_res = NetBlockConvBatchRelu(kernel_size=1, stride=stride,\n",
        "                                              filters=filters2,\n",
        "                                              padding=padding_left,\n",
        "                                              in_channels=first_in_channels)\n",
        "        self.right_res = HALNetResConvSequence(stride=stride,\n",
        "                                               filters1=filters1,\n",
        "                                               filters2=filters2,\n",
        "                                               padding1=padding_right1,\n",
        "                                               padding2=padding_right2,\n",
        "\n",
        "                                               padding3=padding_right3,\n",
        "                                               first_in_channels=\n",
        "                                               first_in_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        left_res = self.left_res(input)\n",
        "        right_res = self.right_res(input)\n",
        "        # element-wise sum\n",
        "        out = left_res + right_res\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "def make_bilinear_weights(size, num_channels):\n",
        "    ''' Make a 2D bilinear kernel suitable for upsampling\n",
        "    Stack the bilinear kernel for application to tensor '''\n",
        "    factor = (size + 1) // 2\n",
        "    if size % 2 == 1:\n",
        "        center = factor - 1\n",
        "    else:\n",
        "        center = factor - 0.5\n",
        "    og = np.ogrid[:size, :size]\n",
        "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
        "           (1 - abs(og[1] - center) / factor)\n",
        "    filt = torch.from_numpy(filt)\n",
        "    w = torch.zeros(num_channels, 1, size, size)\n",
        "    for i in range(num_channels):\n",
        "        w[i, 0] = filt\n",
        "    return w\n",
        "\n",
        "def parse_model_param(params_dict, key, default_value):\n",
        "    try:\n",
        "        ret = params_dict[key]\n",
        "    except:\n",
        "        if default_value == \"Mandatory\":\n",
        "            # raise error again by trying to access value\n",
        "            ret = params_dict[key]\n",
        "        ret = default_value\n",
        "    return ret\n",
        "\n",
        "class HALNet(nn.Module):\n",
        "\n",
        "    cross_entropy = False\n",
        "    joint_ixs = None\n",
        "    use_cuda = None\n",
        "    WEIGHT_LOSS_INTERMED1 = 0.5\n",
        "    WEIGHT_LOSS_INTERMED2 = 0.5\n",
        "    WEIGHT_LOSS_INTERMED3 = 0.5\n",
        "    WEIGHT_LOSS_MAIN = 1\n",
        "\n",
        "    def __init__(self, params_dict):\n",
        "        super(HALNet, self).__init__()\n",
        "        # initialize variables\n",
        "        self.joint_ixs = parse_model_param(params_dict, 'joint_ixs', default_value=\"Mandatory\")\n",
        "        self.use_cuda = parse_model_param(params_dict, 'use_cuda', default_value=False)\n",
        "        self.num_joints = len(self.joint_ixs)\n",
        "        self.cross_entropy = parse_model_param(params_dict, 'cross_entropy', default_value=False)\n",
        "        # build network\n",
        "        self.conv1 = cudafy(NetBlockConvBatchRelu(kernel_size=7, stride=1, filters=64,\n",
        "                                                  in_channels=4, padding=3), self.use_cuda)\n",
        "        self.mp1 = cudafy(nn.MaxPool2d(kernel_size=3, stride=2, padding=1), self.use_cuda)\n",
        "        self.res2a = cudafy(HALNetResBlockConv(stride=1, filters1=64, filters2=256,\n",
        "                                        padding_right1=1), self.use_cuda)\n",
        "        self.res2b = cudafy(HALNetResBlockIDSkip(filters1=64, filters2=256), self.use_cuda)\n",
        "        self.res2c = cudafy(HALNetResBlockIDSkip(filters1=64, filters2=256), self.use_cuda)\n",
        "        self.res3a = cudafy(HALNetResBlockConv(stride=2, filters1=128, filters2=512,\n",
        "                                               padding_right3=1, first_in_channels=256), self.use_cuda)\n",
        "        self.interm_loss1 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=self.num_joints,\n",
        "                                                         in_channels=512, padding=1), self.use_cuda)\n",
        "        self.interm_loss1_deconv = cudafy(nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss1_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.res3b = cudafy(HALNetResBlockIDSkip(filters1=128, filters2=512), self.use_cuda)\n",
        "        self.res3c = cudafy(HALNetResBlockIDSkip(filters1=128, filters2=512), self.use_cuda)\n",
        "        self.res4a = cudafy(HALNetResBlockConv(stride=2, filters1=256, filters2=1024,\n",
        "                                        padding_right3=1,\n",
        "                                        first_in_channels=512), self.use_cuda)\n",
        "        self.interm_loss2 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                         filters=self.num_joints, in_channels=1024,\n",
        "                                                         padding=1), self.use_cuda)\n",
        "        self.interm_loss2_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss2_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.res4b = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.res4c = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.res4d = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.conv4e = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=512,\n",
        "                                                   in_channels=1024, padding=1), self.use_cuda)\n",
        "        self.interm_loss3 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                         filters=self.num_joints, in_channels=512,\n",
        "                                                         padding=1), self.use_cuda)\n",
        "        self.interm_loss3_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss3_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.conv4f = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=256,\n",
        "                                                   in_channels=512, padding=1), self.use_cuda)\n",
        "        self.main_loss_conv = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                           filters=self.num_joints, in_channels=256,\n",
        "                                                           padding=1), self.use_cuda)\n",
        "        self.main_loss_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        if self.cross_entropy:\n",
        "            self.softmax_final = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "\n",
        "    def forward_common_net(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.mp1(out)\n",
        "        out = self.res2a(out)\n",
        "        out = self.res2b(out)\n",
        "        out = self.res2c(out)\n",
        "        res3aout = self.res3a(out)\n",
        "        out = self.res3b(res3aout)\n",
        "        out = self.res3c(out)\n",
        "        res4aout = self.res4a(out)\n",
        "        out = self.res4b(res4aout)\n",
        "        out = self.res4c(out)\n",
        "        out = self.res4d(out)\n",
        "        conv4eout = self.conv4e(out)\n",
        "        conv4fout = self.conv4f(conv4eout)\n",
        "        return res3aout, res4aout, conv4eout, conv4fout\n",
        "\n",
        "    def forward_subnet(self, x):\n",
        "        res3aout, res4aout, conv4eout, conv4fout = self.forward_common_net(x)\n",
        "        # intermediate losses\n",
        "        # intermed 1\n",
        "        out_intermed1 = self.interm_loss1(res3aout)\n",
        "        out_intermed1 = self.interm_loss1_deconv(out_intermed1)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed1 = self.interm_loss1_softmax(out_intermed1)\n",
        "        # intermed 2\n",
        "        out_intermed2 = self.interm_loss2(res4aout)\n",
        "        out_intermed2 = self.interm_loss2_deconv(out_intermed2)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed2 = self.interm_loss2_softmax(out_intermed2)\n",
        "        # intermed 3\n",
        "        out_intermed3 = self.interm_loss3(conv4eout)\n",
        "        out_intermed3 = self.interm_loss3_deconv(out_intermed3)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed3 = self.interm_loss3_softmax(out_intermed3)\n",
        "        return out_intermed1, out_intermed2, out_intermed3, conv4fout,\\\n",
        "               res3aout, res4aout, conv4eout\n",
        "\n",
        "    def forward_main_loss(self, conv4fout):\n",
        "        out = self.main_loss_conv(conv4fout)\n",
        "        out = self.main_loss_deconv(out)\n",
        "        out_main = out\n",
        "        # main loss\n",
        "        if self.cross_entropy:\n",
        "            out_main = self.softmax_final(out)\n",
        "        return out_main\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get subhalnet outputs (common to JORNet)\n",
        "        out_intermed1, out_intermed2, out_intermed3, conv4fout, _, _, _ = self.forward_subnet(x)\n",
        "        # out to main loss of halnet\n",
        "        out_main = self.forward_main_loss(conv4fout)\n",
        "        return out_intermed1, out_intermed2, out_intermed3, out_main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB-sQpum-sHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HALNet(nn.Module):\n",
        "\n",
        "    cross_entropy = False\n",
        "    joint_ixs = None\n",
        "    use_cuda = None\n",
        "    WEIGHT_LOSS_INTERMED1 = 0.5\n",
        "    WEIGHT_LOSS_INTERMED2 = 0.5\n",
        "    WEIGHT_LOSS_INTERMED3 = 0.5\n",
        "    WEIGHT_LOSS_MAIN = 1\n",
        "\n",
        "    def __init__(self):\n",
        "        super(HALNet, self).__init__()\n",
        "        # initialize variables\n",
        "#         self.joint_ixs = parse_model_param(params_dict, 'joint_ixs', default_value=\"Mandatory\")\n",
        "        self.use_cuda = True #parse_model_param(params_dict, 'use_cuda', default_value=False)\n",
        "        self.num_joints = 21#len(self.joint_ixs)\n",
        "        self.cross_entropy =  nn.CrossEntropyLoss()# parse_model_param(params_dict, 'cross_entropy', default_value=False)\n",
        "        # build network\n",
        "        self.conv1 = cudafy(NetBlockConvBatchRelu(kernel_size=7, stride=1, filters=64,\n",
        "                                                  in_channels=4, padding=3), self.use_cuda)\n",
        "        self.mp1 = cudafy(nn.MaxPool2d(kernel_size=3, stride=2, padding=1), self.use_cuda)\n",
        "        self.res2a = cudafy(HALNetResBlockConv(stride=1, filters1=64, filters2=256,\n",
        "                                        padding_right1=1), self.use_cuda)\n",
        "        self.res2b = cudafy(HALNetResBlockIDSkip(filters1=64, filters2=256), self.use_cuda)\n",
        "        self.res2c = cudafy(HALNetResBlockIDSkip(filters1=64, filters2=256), self.use_cuda)\n",
        "        self.res3a = cudafy(HALNetResBlockConv(stride=2, filters1=128, filters2=512,\n",
        "                                               padding_right3=1, first_in_channels=256), self.use_cuda)\n",
        "        self.interm_loss1 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=self.num_joints,\n",
        "                                                         in_channels=512, padding=1), self.use_cuda)\n",
        "        self.interm_loss1_deconv = cudafy(nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss1_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.res3b = cudafy(HALNetResBlockIDSkip(filters1=128, filters2=512), self.use_cuda)\n",
        "        self.res3c = cudafy(HALNetResBlockIDSkip(filters1=128, filters2=512), self.use_cuda)\n",
        "        self.res4a = cudafy(HALNetResBlockConv(stride=2, filters1=256, filters2=1024,\n",
        "                                        padding_right3=1,\n",
        "                                        first_in_channels=512), self.use_cuda)\n",
        "        self.interm_loss2 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                         filters=self.num_joints, in_channels=1024,\n",
        "                                                         padding=1), self.use_cuda)\n",
        "        self.interm_loss2_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss2_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.res4b = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.res4c = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.res4d = cudafy(HALNetResBlockIDSkip(filters1=256, filters2=1024), self.use_cuda)\n",
        "        self.conv4e = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=512,\n",
        "                                                   in_channels=1024, padding=1), self.use_cuda)\n",
        "        self.interm_loss3 = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                         filters=self.num_joints, in_channels=512,\n",
        "                                                         padding=1), self.use_cuda)\n",
        "        self.interm_loss3_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        self.interm_loss3_softmax = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.conv4f = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1, filters=256,\n",
        "                                                   in_channels=512, padding=1), self.use_cuda)\n",
        "        self.main_loss_conv = cudafy(NetBlockConvBatchRelu(kernel_size=3, stride=1,\n",
        "                                                           filters=self.num_joints, in_channels=256,\n",
        "                                                           padding=1), self.use_cuda)\n",
        "        self.main_loss_deconv = cudafy(nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True), self.use_cuda)\n",
        "        if self.cross_entropy:\n",
        "            self.softmax_final = cudafy(NetBlockSoftmaxLogProbability2D(), self.use_cuda)\n",
        "\n",
        "    def forward_common_net(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.mp1(out)\n",
        "        out = self.res2a(out)\n",
        "        out = self.res2b(out)\n",
        "        out = self.res2c(out)\n",
        "        res3aout = self.res3a(out)\n",
        "        out = self.res3b(res3aout)\n",
        "        out = self.res3c(out)\n",
        "        res4aout = self.res4a(out)\n",
        "        out = self.res4b(res4aout)\n",
        "        out = self.res4c(out)\n",
        "        out = self.res4d(out)\n",
        "        conv4eout = self.conv4e(out)\n",
        "        conv4fout = self.conv4f(conv4eout)\n",
        "        return res3aout, res4aout, conv4eout, conv4fout\n",
        "\n",
        "    def forward_subnet(self, x):\n",
        "        res3aout, res4aout, conv4eout, conv4fout = self.forward_common_net(x)\n",
        "        # intermediate losses\n",
        "        # intermed 1\n",
        "        out_intermed1 = self.interm_loss1(res3aout)\n",
        "        out_intermed1 = self.interm_loss1_deconv(out_intermed1)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed1 = self.interm_loss1_softmax(out_intermed1)\n",
        "        # intermed 2\n",
        "        out_intermed2 = self.interm_loss2(res4aout)\n",
        "        out_intermed2 = self.interm_loss2_deconv(out_intermed2)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed2 = self.interm_loss2_softmax(out_intermed2)\n",
        "        # intermed 3\n",
        "        out_intermed3 = self.interm_loss3(conv4eout)\n",
        "        out_intermed3 = self.interm_loss3_deconv(out_intermed3)\n",
        "        if self.cross_entropy:\n",
        "            out_intermed3 = self.interm_loss3_softmax(out_intermed3)\n",
        "        return out_intermed1, out_intermed2, out_intermed3, conv4fout,\\\n",
        "               res3aout, res4aout, conv4eout\n",
        "\n",
        "    def forward_main_loss(self, conv4fout):\n",
        "        out = self.main_loss_conv(conv4fout)\n",
        "        out = self.main_loss_deconv(out)\n",
        "        out_main = out\n",
        "        # main loss\n",
        "        if self.cross_entropy:\n",
        "            out_main = self.softmax_final(out)\n",
        "        return out_main\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get subhalnet outputs (common to JORNet)\n",
        "        out_intermed1, out_intermed2, out_intermed3, conv4fout, _, _, _ = self.forward_subnet(x)\n",
        "        # out to main loss of halnet\n",
        "        out_main = self.forward_main_loss(conv4fout)\n",
        "        return out_intermed1, out_intermed2, out_intermed3, out_main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XSX2MtBxwP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dims_fun(w, h):\n",
        "  \n",
        "    w, h = (w-7+2*3)/1+1, (h-7+2*3)/1+1 #conv1\n",
        "    w, h = (w-3+2*1)/2+1, (h-3+2*1)/2+1 #maxpool\n",
        "    w, h = (w-1+2*0)/1+1, (h-1+2*0)/1+1 #res2out\n",
        "    w, h = (w-1+2*0)/2+1, (h-1+2*0)/2+1 #res3out\n",
        "    w,h = int(w),int(h)\n",
        "    res3out = w * h * 512\n",
        "   \n",
        "    w, h = (w-1+2*0)/2+1, (h-1+2*0)/2+1 #res4out\n",
        "    w,h = int(w),int(h)\n",
        "    res4out = w * h * 1024\n",
        "   \n",
        "    w, h = (w-3+2*1)/1+1, (h-3+2*1)/1+1 #conv4e\n",
        "    w,h = int(w),int(h)\n",
        "    conv4e = w*h*512\n",
        "\n",
        "    w, h = (w-3+2*1)/1+1, (h-3+2*1)/1+1 #conv4f\n",
        "    w,h = int(w),int(h)\n",
        "    conv4f = w*h*256\n",
        "    return res3out,res4out,conv4e,conv4f,h,w\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z59dFZ7D0JYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class JORNet(HALNet):\n",
        "\n",
        "    #innerprod1_size = 65536\n",
        "\n",
        "    def map_out_to_loss(self, innerprod1_size):\n",
        "        return cudafy(nn.Linear(in_features=innerprod1_size, out_features=200), self.use_cuda)\n",
        "\n",
        "    def map_out_conv(self, in_channels):\n",
        "        return cudafy(HALNet.HALNetConvBlock(\n",
        "            kernel_size=3, stride=1, filters=21, in_channels=in_channels, padding=1),\n",
        "            self.use_cuda)\n",
        "\n",
        "    def __init__(self,dims,num_joints=20):\n",
        "        super(JORNet, self).__init__()\n",
        "        \n",
        "        res3out,res4out,conv4e,conv4f,h,w = dims_fun(dims[0],dims[1])\n",
        "#         self.innerprod1_size = 256 * h * w\n",
        "        self.crop_res = dims\n",
        "        self.num_joints = num_joints\n",
        "#         self.main_loss_conv = cudafy(HALNet.HALNetConvBlock(\n",
        "#                 kernel_size=3, stride=1, filters=21, in_channels=256, padding=1),\n",
        "#             self.use_cuda)\n",
        "        self.main_loss_deconv1 = cudafy(nn.Upsample(size=self.crop_res, mode='bilinear'), self.use_cuda)\n",
        "        #self.main_loss_deconv2 = cudafy(nn.Upsample(scale_factor=1, mode='bilinear'),\n",
        "        #                                self.use_cuda)\n",
        "        if self.cross_entropy:\n",
        "            self.softmax_final = cudafy(HALNet.\n",
        "                                        SoftmaxLogProbability2D(), self.use_cuda)\n",
        "        self.innerproduct1_joint1 = cudafy(\n",
        "            nn.Linear(in_features=res3out, out_features=200), self.use_cuda)\n",
        "        self.innerproduct2_joint1 = cudafy(\n",
        "            nn.Linear(in_features=200, out_features=self.num_joints * 3), self.use_cuda)\n",
        "\n",
        "        self.innerproduct1_joint2 = cudafy(\n",
        "            nn.Linear(in_features=res4out, out_features=200), self.use_cuda)\n",
        "        self.innerproduct2_joint2 = cudafy(\n",
        "            nn.Linear(in_features=200, out_features=self.num_joints * 3), self.use_cuda)\n",
        "\n",
        "        self.innerproduct1_joint3 = cudafy(\n",
        "            nn.Linear(in_features=conv4e, out_features=200), self.use_cuda)\n",
        "        self.innerproduct2_joint3 = cudafy(\n",
        "            nn.Linear(in_features=200, out_features=self.num_joints * 3), self.use_cuda)\n",
        "\n",
        "        self.innerproduct1_joint_main = cudafy(\n",
        "            nn.Linear(in_features=conv4f, out_features=200), self.use_cuda)\n",
        "        self.innerproduct2_join_main = cudafy(\n",
        "            nn.Linear(in_features=200, out_features=self.num_joints * 3), self.use_cuda)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_intermed_hm1, out_intermed_hm2, out_intermed_hm3, conv4fout, \\\n",
        "        res3aout, res4aout, conv4eout = self.forward_subnet(x)\n",
        "        out_intermed_hm_main = self.forward_main_loss(conv4fout)\n",
        "        innerprod1_size = res3aout.shape[1] * res3aout.shape[2] * res3aout.shape[3]\n",
        "        out_intermed_j1 = res3aout.view(-1, innerprod1_size)\n",
        "        out_intermed_j1 = self.innerproduct1_joint1(out_intermed_j1)\n",
        "        out_intermed_j1 = self.innerproduct2_joint1(out_intermed_j1)\n",
        "\n",
        "        innerprod1_size = res4aout.shape[1] * res4aout.shape[2] * res4aout.shape[3]\n",
        "        out_intermed_j2 = res4aout.view(-1, innerprod1_size)\n",
        "        out_intermed_j2 = self.innerproduct1_joint2(out_intermed_j2)\n",
        "        out_intermed_j2 = self.innerproduct2_joint2(out_intermed_j2)\n",
        "\n",
        "        innerprod1_size = conv4eout.shape[1] * conv4eout.shape[2] * conv4eout.shape[3]\n",
        "        out_intermed_j3 = conv4eout.view(-1, innerprod1_size)\n",
        "        out_intermed_j3 = self.innerproduct1_joint3(out_intermed_j3)\n",
        "        out_intermed_j3 = self.innerproduct2_joint3(out_intermed_j3)\n",
        "\n",
        "        innerprod1_size = conv4fout.shape[1] * conv4fout.shape[2] * conv4fout.shape[3]\n",
        "        out_intermed_j_main = conv4fout.view(-1, innerprod1_size)\n",
        "        out_intermed_j_main = self.innerproduct1_joint_main(out_intermed_j_main)\n",
        "        out_intermed_j_main = self.innerproduct2_join_main(out_intermed_j_main)\n",
        "\n",
        "        return out_intermed_hm1, out_intermed_hm2, out_intermed_hm3, out_intermed_hm_main,\\\n",
        "               out_intermed_j1, out_intermed_j2, out_intermed_j3, out_intermed_j_main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6AO_GvhpRs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# halnet = HALNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IknGMTZupbTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = halnet.forward_common_net(rgbd_img[None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up-5Kidq7-Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dims = (128,250)\n",
        "rgbd_img = torch.rand((4,dims[0],dims[1]))\n",
        "rgbd_img = rgbd_img.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q47UOPK0at6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jornet = JORNet(dims,num_joints=21)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5s9uk0Q0tts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# jornet(rgbd_img[None])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fJpzcr5OybD",
        "colab_type": "code",
        "outputId": "27add156-bf24-4526-c586-905a18d55121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from time import time\n",
        "i = 0\n",
        "with torch.no_grad():\n",
        "  \n",
        "    t = time()\n",
        "    while True:\n",
        "        i+=1\n",
        "        jornet(rgbd_img[None])\n",
        "        \n",
        "        if time()-t>1:      \n",
        "            print(\"time: {} s\".format(time()-t))\n",
        "            print(\"number of feedforward: {}\".format(i))\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.0158119201660156 s\n",
            "number of feedforward: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfmZ-igF5XK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdcitfGP5ckY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}